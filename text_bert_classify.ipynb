{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, BertForSequenceClassification, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from sklearn import metrics,utils\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "print(data.describe())\n",
    "plt.hist(data['label'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "classes = np.sort(data['label'].drop_duplicates().values)\n",
    "classes_dict = {}\n",
    "for i, cls in enumerate(classes):\n",
    "    classes_dict[str(cls)] = i\n",
    "classes_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_labels = len(classes_dict)\n",
    "print(num_labels)\n",
    "\n",
    "data['label'] = data['label'].map(lambda x: classes_dict[str(x)])\n",
    "data['label'] = data['label'].map(lambda x: 1 if x>4  else 0)\n",
    "\n",
    "max_len = [len(line.split(' ')) for line in data['text']]\n",
    "print(max(max_len))\n",
    "data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(data['label'], bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(max_len, bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(classes)}\n",
    "label2id = {label: str(i) for i, label in enumerate(classes)}\n",
    "\n",
    "dataset = Dataset.from_pandas(data, preserve_index=False)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name='sberbank-ai/ruBert-base'\n",
    "# model_name = 'distilbert-base-multilingual-cased'\n",
    "# model_name='distilbert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, model_max_length=32)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config._num_labels = len(id2label)\n",
    "model.config.num_labels = len(id2label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_weights = utils.class_weight.compute_class_weight(class_weight ='balanced',  classes=range(0,2), y=dataset['train']['label'])\n",
    "class_weights=np.array(class_weights).astype(np.float32)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,weight_decay=1e-3)\n",
    "device = 'cuda'\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(class_weights).to(device))\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "for i in trange(epochs, desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n",
    "        labels = torch.from_numpy(np.asarray(batch['label'])).to(device)\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        input_ids = torch.stack((input_ids)).to(device)\n",
    "        input_ids = input_ids.permute(1, 0)\n",
    "\n",
    "        attention_mask = batch['attention_mask']\n",
    "        attention_mask = torch.stack((attention_mask)).to(device)\n",
    "        attention_mask = attention_mask.permute(1, 0)\n",
    "\n",
    "        output = model(input_ids, attention_mask, token_type_ids=None, labels=labels)\n",
    "        logits = output['logits']\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        msg = f'Epoch: {i + 1}/{epochs}\\tStep: {step + 1}/{len(train_dataloader)}\\tLoss: {tr_loss / (step + 1):.3f}\\n'\n",
    "        print(msg, end='')\n",
    "\n",
    "    tr_loss_test = 0\n",
    "    with torch.no_grad():\n",
    "        true_predictions=[]\n",
    "        true_labels=[]\n",
    "        for step, batch in enumerate(tqdm_notebook(test_dataloader, desc=\"Iteration\")):\n",
    "            labels = torch.from_numpy(np.asarray(batch['label'])).to(device)\n",
    "\n",
    "            input_ids = batch['input_ids']\n",
    "            input_ids = torch.stack((input_ids)).to(device)\n",
    "            input_ids = input_ids.permute(1, 0)\n",
    "\n",
    "            attention_mask = batch['attention_mask']\n",
    "            attention_mask = torch.stack((attention_mask)).to(device)\n",
    "            attention_mask = attention_mask.permute(1, 0)\n",
    "\n",
    "            output = model(input_ids, attention_mask, token_type_ids=None, labels=labels)\n",
    "            logits = output['logits']\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            true_predictions.extend(logits.detach().cpu().numpy())\n",
    "            true_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            tr_loss_test += loss.item()\n",
    "\n",
    "    preds=np.argmax(true_predictions,axis=1).reshape((-1,1))\n",
    "\n",
    "    # results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # msg = f'Epoch: {i + 1}/{epochs}\\t Validation Loss: {tr_loss_test / (len(test_dataloader)):.3f}\\tRecall{results[\"overall_recall\"]:.3f}\\t' \\\n",
    "    #       f'Precision: {results[\"overall_precision\"]:.3f}\\tAccuracy: {results[\"overall_accuracy\"]:.3f}\\tF1: {results[\"overall_f1\"]:.3f} \\n'\n",
    "\n",
    "    print(classification_report(true_labels,preds))\n",
    "    # print(msg, end='')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finetune with  trainer (not completed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True):\n",
    "    y_pred = torch.from_numpy(y_pred)\n",
    "    y_true = torch.from_numpy(y_true)\n",
    "    if sigmoid:\n",
    "        y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred > thresh) == y_true.bool()).float().mean().item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {'accuracy_thresh': accuracy_thresh(predictions, labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-classify-{str(time.time())}\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=1e-3,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,\n",
    "    report_to='wandb',\n",
    "    # fp16=True,\n",
    "    weight_decay=1e-5,\n",
    "    dataloader_num_workers=4,\n",
    "    metric_for_best_model='accuracy'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}