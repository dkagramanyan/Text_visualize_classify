{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch import nn\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, DataCollatorWithPadding,DistilBertForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_excel('DataML3.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df[['ConsumerFactor','DEFECT_DESC','QHC_FUP_SERVICE_TYPE','OverallSatisfactionScore']].to_csv('DataML3.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              label\n",
      "count  36003.000000\n",
      "mean       0.946645\n",
      "std        0.157922\n",
      "min        0.000000\n",
      "25%        1.000000\n",
      "50%        1.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv')\n",
    "print(data.describe())\n",
    "plt.hist(data['label'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": "   label                                              text\n0      8                        Нет изображения на дисплее\n1      7  недостаточное замораживание в морозильной камере\n2      8                    не работает верхний вентилятор\n3      8                                       Отключается\n4      8               Дисплей/ нет изображения/ постоянно",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>Нет изображения на дисплее</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>недостаточное замораживание в морозильной камере</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>не работает верхний вентилятор</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>Отключается</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>Дисплей/ нет изображения/ постоянно</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv')\n",
    "classes=np.sort(data['label'].drop_duplicates().values)\n",
    "classes_dict={}\n",
    "for i,cls in enumerate(classes):\n",
    "    classes_dict[str(cls)]=i\n",
    "print(len(classes_dict))\n",
    "data['label']=data['label'].map(lambda x: classes_dict[str(x)])\n",
    "\n",
    "max_len=[len(line.split(' ')) for line in data['text']]\n",
    "print(max(max_len))\n",
    "data.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(max_len,bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "id2label = {str(i):label for i, label in enumerate(classes)}\n",
    "label2id = {label:str(i) for i, label in enumerate(classes)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data,preserve_index=False)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# model_name='sberbank-ai/ruBert-base'\n",
    "model_name='distilbert-base-multilingual-cased'\n",
    "# model_name='distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,model_max_length=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config._num_labels = len(id2label)\n",
    "model.config.num_labels = len(id2label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/29 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ae10423953449748a6d169477bd4f0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ae640b99e08404ebed3ed6d4bbd6233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "{'label': 8,\n 'text': 'Не работае левый наушник.',\n 'input_ids': [101,\n  21124,\n  27566,\n  10205,\n  94693,\n  50941,\n  10122,\n  38368,\n  11718,\n  119,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "class DistilBertForMultilabelSequenceClassification(DistilBertForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                return_dict=None):\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.distilbert(input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  head_mask=head_mask,\n",
    "                                  inputs_embeds=inputs_embeds,\n",
    "                                  output_attentions=output_attentions,\n",
    "                                  output_hidden_states=output_hidden_states,\n",
    "                                  return_dict=return_dict)\n",
    "\n",
    "        hidden_state = outputs[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        _, pred = torch.max(logits, 1)\n",
    "\n",
    "        # print(logits)\n",
    "        # print(pred)\n",
    "        # print(labels)\n",
    "        # loss = loss_fct(logits.view(-1, self.num_labels), labels.float().view(-1, self.num_labels))\n",
    "        loss = loss_fct(pred.float(), labels.float())\n",
    "        # print(loss)\n",
    "\n",
    "        loss = Variable(loss, requires_grad = True)\n",
    "\n",
    "        # loss = self.criterion(logits.unsqueeze(1).float(), labels.float()).to('cuda')\n",
    "\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss,\n",
    "                                        logits=logits,\n",
    "                                        hidden_states=outputs.hidden_states,\n",
    "                                        attentions=outputs.attentions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Администратор/.cache\\huggingface\\transformers\\cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Администратор/.cache\\huggingface\\transformers\\7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForMultilabelSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForMultilabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForMultilabelSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels=9\n",
    "model_ckpt='distilbert-base-multilingual-cased'\n",
    "model = DistilBertForMultilabelSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to('cuda')\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config._num_labels = len(id2label)\n",
    "model.config.num_labels = len(id2label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(input_data):\n",
    "    true_predictions, true_labels = input_data\n",
    "    # predictions = np.argmax(predictions, axis=2)\n",
    "    #\n",
    "    # true_predictions = [[p for (p, l) in zip(prediction, label) if l != -100] for prediction, label in\n",
    "    #                     zip(predictions, labels)]\n",
    "    # true_labels = [[l for (p, l) in zip(prediction, label) if l != -100] for prediction, label in\n",
    "    #                zip(predictions, labels)]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # print(results)\n",
    "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metric = load_metric(\"accuracy\")\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # if task != \"stsb\":\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # else:\n",
    "    # predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "epochs=4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-classify-{str(time.time())}\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=1e-3,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    logging_strategy='steps',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,\n",
    "    report_to='wandb',\n",
    "    # fp16=True,\n",
    "    weight_decay=1e-5,\n",
    "    dataloader_num_workers=4,\n",
    "    metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     f\"test-classify-{str(time.time())}\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     learning_rate=1e-3,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=epochs,\n",
    "#     logging_steps=10,\n",
    "#     weight_decay=1e-2,\n",
    "#     report_to='wandb'\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18b0b31e43114f67b86ea82a61b102b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">test-classify-1670348921.4530172</strong>: <a href=\"https://wandb.ai/dkagramanyan/huggingface/runs/a7e6ewir\" target=\"_blank\">https://wandb.ai/dkagramanyan/huggingface/runs/a7e6ewir</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20221206_204846-a7e6ewir\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForMultilabelSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 28802\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7204\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='7204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/7204 : < :, Epoch 0.00/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMultilabelSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-107-3435b262f1ae>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1412\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1413\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrol\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcallback_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_epoch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1414\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_log_save_evaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtr_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrial\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_keys_for_eval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1415\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1416\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mDebugOption\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTPU_METRICS_DEBUG\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36m_maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1519\u001B[0m         \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1520\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_evaluate\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1521\u001B[1;33m             \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mignore_keys\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mignore_keys_for_eval\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1522\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_report_to_hp_search\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1523\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mevaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   2156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2157\u001B[0m         \u001B[0meval_loop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprediction_loop\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muse_legacy_prediction_loop\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluation_loop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2158\u001B[1;33m         output = eval_loop(\n\u001B[0m\u001B[0;32m   2159\u001B[0m             \u001B[0meval_dataloader\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2160\u001B[0m             \u001B[0mdescription\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"Evaluation\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mevaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   2399\u001B[0m         \u001B[1;31m# Metrics!\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2400\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mall_preds\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mall_labels\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2401\u001B[1;33m             \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_metrics\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mEvalPrediction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mall_preds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_ids\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mall_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2402\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2403\u001B[0m             \u001B[0mmetrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-50-869b9d17a525>\u001B[0m in \u001B[0;36mcompute_metrics\u001B[1;34m(input_data)\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;31m#                zip(predictions, labels)]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmetric\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrue_predictions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreferences\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrue_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[1;31m# print(results)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m     return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"],\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\metric.py\u001B[0m in \u001B[0;36mcompute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    440\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0many\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 442\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    443\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_finalize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\metric.py\u001B[0m in \u001B[0;36madd_batch\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    492\u001B[0m         \u001B[0mbatch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"predictions\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"references\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mreferences\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    493\u001B[0m         \u001B[0mbatch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mintput_name\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mintput_name\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mintput_name\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 494\u001B[1;33m         \u001B[0mbatch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    495\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    496\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_init_writer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\features\\features.py\u001B[0m in \u001B[0;36mencode_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m   1773\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1774\u001B[0m             \u001B[0mcolumn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcast_to_python_objects\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1775\u001B[1;33m             \u001B[0mencoded_batch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mencode_nested_example\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1776\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mencoded_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1777\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\features\\features.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1773\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolumn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1774\u001B[0m             \u001B[0mcolumn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcast_to_python_objects\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolumn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1775\u001B[1;33m             \u001B[0mencoded_batch\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mencode_nested_example\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcolumn\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1776\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mencoded_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1777\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\envs\\torch\\lib\\site-packages\\datasets\\features\\features.py\u001B[0m in \u001B[0;36mencode_nested_example\u001B[1;34m(schema, obj, level)\u001B[0m\n\u001B[0;32m   1206\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Got a string but expected a list instead: '{obj}'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1207\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1208\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1209\u001B[0m                 \u001B[1;32mfor\u001B[0m \u001B[0mfirst_elmt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1210\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0m_check_non_null_non_empty_recursive\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfirst_elmt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_datasets['test'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(\"mazur\")\n",
    "tokenizer.save_pretrained(\"mazur\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the model/tokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(model=model)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "def pipeline_prediction(text):\n",
    "    df=pd.DataFrame({'text':[text]})\n",
    "    dataset = Dataset.from_pandas(df,preserve_index=False)\n",
    "    tokenized_datasets = dataset.map(tokenize_function)\n",
    "    raw_pred, _, _ = trainer.predict(tokenized_datasets)\n",
    "    return(raw_pred[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}